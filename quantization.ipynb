{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d46cb0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchinfo in /jet/home/psamal/hw_envs/idl_hw4/lib/python3.12/site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install transformers==4.31.0 accelerate\n",
    "# !pip install \"nvidia-modelopt[all]\" -U --extra-index-url https://pypi.nvidia.com\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fb3180a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.5.1 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (4.13.2)\n",
      "Requirement already satisfied: networkx in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Collecting triton==3.1.0 (from torch==2.5.1)\n",
      "  Using cached triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from torch==2.5.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /jet/home/fxiang1/.conda/envs/quant/lib/python3.11/site-packages (from jinja2->torch==2.5.1) (3.0.2)\n",
      "Using cached triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "Installing collected packages: triton\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.21.0 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed triton-3.1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch==2.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "835159ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c882efed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local\n"
     ]
    }
   ],
   "source": [
    "!echo $LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eca1a1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!echo $HF_HOME\n",
    "# !export HF_HOME=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78313c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7580aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, modelopt.torch.quantization as mtq\n",
    "cfg = copy.deepcopy(mtq.INT4_AWQ_CFG)\n",
    "cfg[\"quant_cfg\"][\"*lm_head*\"] = {\"enable\": False}   # keep logits full‑precision :contentReference[oaicite:2]{index=2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e72bc716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch, random\n",
    "\n",
    "raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "texts = random.sample(list(raw[\"text\"]), k=128)       # ~50 k tokens\n",
    "def collate(batch):\n",
    "    ids = tok(batch, return_tensors=\"pt\", padding=True, truncation=True,\n",
    "              max_length=2048)[\"input_ids\"].to(model.device)\n",
    "    return ids\n",
    "calib_loader = torch.utils.data.DataLoader(texts, batch_size=2, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecb38f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================================================================================================\n",
       "Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
       "==========================================================================================================================================================================\n",
       "Qwen2ForCausalLM                              [1, 128]                  --                        --                        --                        --\n",
       "├─Qwen2Model: 1-1                             --                        --                        --                        --                        --\n",
       "│    └─Embedding: 2-1                         [1, 128]                  [1, 128, 1536]            233,373,696               --                        233,373,696\n",
       "│    └─Qwen2RotaryEmbedding: 2-2              [1, 128, 1536]            [1, 128, 128]             --                        --                        --\n",
       "│    └─ModuleList: 2-3                        --                        --                        --                        --                        --\n",
       "│    │    └─Qwen2DecoderLayer: 3-1            [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-2            [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-3            [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-4            [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-5            [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-6            [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-7            [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-8            [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-9            [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-10           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-11           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-12           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-13           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-14           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-15           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-16           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-17           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-18           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-19           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-20           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-21           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-22           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-23           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-24           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-25           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-26           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-27           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-28           [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        46,797,824\n",
       "│    └─Qwen2RMSNorm: 2-4                      [1, 128, 1536]            [1, 128, 1536]            1,536                     --                        1,536\n",
       "├─Linear: 1-2                                 [1, 128, 1536]            [1, 128, 151936]          233,373,696               --                        233,373,696\n",
       "==========================================================================================================================================================================\n",
       "Total params: 1,777,088,000\n",
       "Trainable params: 1,777,088,000\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.78\n",
       "==========================================================================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 453.71\n",
       "Params size (MB): 3554.18\n",
       "Estimated Total Size (MB): 4007.88\n",
       "=========================================================================================================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len  = 128\n",
    "batch    = torch.randint(\n",
    "              0, model.config.vocab_size,        # token IDs\n",
    "              (1, seq_len),                       # (B, L)\n",
    "              dtype=torch.long,\n",
    "              device=device)\n",
    "summary(model, input_data=batch, col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2dc39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition of _QuantQwen2Attention saved to /var/tmp/modelopt_asracd02.py\n",
      "Successfully registered Qwen2Attention for quantization\n",
      "Inserted 675 quantizers\n",
      "Caching activation statistics for awq_lite...\n",
      "Searching awq_lite parameters...\n",
      "Loading extension modelopt_cuda_ext...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/fxiang1/.local/lib/python3.12/site-packages/modelopt/torch/utils/cpp_extension.py:88: UserWarning: Error building extension 'modelopt_cuda_ext': [1/3] /opt/packages/cuda/v12.6.1/bin/nvcc --generate-dependencies-with-compile --dependency-output tensor_quant_gpu.cuda.o.d -DTORCH_EXTENSION_NAME=modelopt_cuda_ext -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include -isystem /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/TH -isystem /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/THC -isystem /opt/packages/cuda/v12.6.1/include -isystem /jet/home/psamal/hw_envs/idl_hw4/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -std=c++17 -c /jet/home/fxiang1/.local/lib/python3.12/site-packages/modelopt/torch/quantization/src/tensor_quant_gpu.cu -o tensor_quant_gpu.cuda.o \n",
      "\u001b[31mFAILED: \u001b[0mtensor_quant_gpu.cuda.o \n",
      "/opt/packages/cuda/v12.6.1/bin/nvcc --generate-dependencies-with-compile --dependency-output tensor_quant_gpu.cuda.o.d -DTORCH_EXTENSION_NAME=modelopt_cuda_ext -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include -isystem /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/TH -isystem /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/THC -isystem /opt/packages/cuda/v12.6.1/include -isystem /jet/home/psamal/hw_envs/idl_hw4/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -std=c++17 -c /jet/home/fxiang1/.local/lib/python3.12/site-packages/modelopt/torch/quantization/src/tensor_quant_gpu.cu -o tensor_quant_gpu.cuda.o \n",
      "In file included from /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/c10/util/CallOnce.h:4,\n",
      "                 from /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/ATen/Context.h:24,\n",
      "                 from /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/ATen/ATen.h:7,\n",
      "                 from /jet/home/fxiang1/.local/lib/python3.12/site-packages/modelopt/torch/quantization/src/tensor_quant_gpu.cu:18:\n",
      "/jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error \"You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later.\"\n",
      " #error \\\n",
      "  ^~~~~\n",
      "/jet/home/fxiang1/.local/lib/python3.12/site-packages/modelopt/torch/quantization/src/tensor_quant_gpu.cu:28: warning: \"AT_DISPATCH_CASE_FLOATING_TYPES\" redefined\n",
      " #define AT_DISPATCH_CASE_FLOATING_TYPES(...)                                                       \\\n",
      " \n",
      "In file included from /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/ATen/ATen.h:11,\n",
      "                 from /jet/home/fxiang1/.local/lib/python3.12/site-packages/modelopt/torch/quantization/src/tensor_quant_gpu.cu:18:\n",
      "/jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/ATen/Dispatch.h:214: note: this is the location of the previous definition\n",
      " #define AT_DISPATCH_CASE_FLOATING_TYPES(...)            \\\n",
      " \n",
      "[2/3] c++ -MMD -MF tensor_quant.o.d -DTORCH_EXTENSION_NAME=modelopt_cuda_ext -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include -isystem /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/TH -isystem /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/THC -isystem /opt/packages/cuda/v12.6.1/include -isystem /jet/home/psamal/hw_envs/idl_hw4/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /jet/home/fxiang1/.local/lib/python3.12/site-packages/modelopt/torch/quantization/src/tensor_quant.cpp -o tensor_quant.o \n",
      "\u001b[31mFAILED: \u001b[0mtensor_quant.o \n",
      "c++ -MMD -MF tensor_quant.o.d -DTORCH_EXTENSION_NAME=modelopt_cuda_ext -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include -isystem /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/TH -isystem /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/THC -isystem /opt/packages/cuda/v12.6.1/include -isystem /jet/home/psamal/hw_envs/idl_hw4/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /jet/home/fxiang1/.local/lib/python3.12/site-packages/modelopt/torch/quantization/src/tensor_quant.cpp -o tensor_quant.o \n",
      "In file included from /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/ATen/core/TensorBase.h:14,\n",
      "                 from /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/ATen/core/TensorBody.h:38,\n",
      "                 from /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/ATen/core/Tensor.h:3,\n",
      "                 from /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                 from /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
      "                 from /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
      "                 from /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
      "                 from /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
      "                 from /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
      "                 from /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
      "                 from /jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/torch/extension.h:5,\n",
      "                 from /jet/home/fxiang1/.local/lib/python3.12/site-packages/modelopt/torch/quantization/src/tensor_quant.h:18,\n",
      "                 from /jet/home/fxiang1/.local/lib/python3.12/site-packages/modelopt/torch/quantization/src/tensor_quant.cpp:18:\n",
      "/jet/home/fxiang1/.local/lib/python3.12/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error \"You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later.\"\n",
      " #error \\\n",
      "  ^~~~~\n",
      "ninja: build stopped: subcommand failed.\n",
      "\n",
      "Unable to load extension modelopt_cuda_ext and falling back to CPU version.\n",
      "  warnings.warn(fail_msg)\n"
     ]
    }
   ],
   "source": [
    "import modelopt.torch.quantization as mtq\n",
    "\n",
    "model_pre_q = copy.deepcopy(model).to(device)\n",
    "\n",
    "def forward_loop(m, max_batch = 1000):                 # PTQ forward pass\n",
    "    for batch_idx, ids in enumerate(calib_loader):\n",
    "        m(ids)\n",
    "        if batch_idx >= max_batch:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "qmodel = mtq.quantize(model_pre_q, cfg, forward_loop)       # :contentReference[oaicite:3]{index=3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a8440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ModelOpt state to deepseek-qwen1.5b-awq-int4/modelopt_state.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('deepseek-qwen1.5b-awq-int4/tokenizer_config.json',\n",
       " 'deepseek-qwen1.5b-awq-int4/special_tokens_map.json',\n",
       " 'deepseek-qwen1.5b-awq-int4/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = \"deepseek-qwen1.5b-awq-int4\"\n",
    "qmodel.save_pretrained(out_dir)\n",
    "tok.save_pretrained(out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5426e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0173, 1.2783](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.0.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0236, 0.4045](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.0.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0276, 0.1754](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.0.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0175, 1.0557](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.0.self_attn.q_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.k_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.0.self_attn.v_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.0.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.0.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.0.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0641, 1.2852](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.0.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.0.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.0.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0399, 0.6108](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.0.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.0.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.0.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0294, 0.6587](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.1.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0173, 0.3628](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.1.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0290, 0.5640](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.1.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0308, 0.3481](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.1.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0284, 0.6001](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.1.self_attn.q_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.k_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.1.self_attn.v_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.1.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.1.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.1.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0239, 1.2900](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.1.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.1.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.1.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0051, 0.7441](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.1.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.1.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.1.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0187, 3.3691](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.2.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0214, 0.4224](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.2.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0258, 0.3081](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.2.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0139, 0.1107](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.2.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0365, 0.5518](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.2.self_attn.q_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.k_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.2.self_attn.v_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.2.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.2.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.2.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0645, 0.9131](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.2.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.2.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.2.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0189, 0.6499](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.2.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.2.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.2.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0061, 7.6719](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.3.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0322, 0.4673](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.3.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0241, 0.3267](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.3.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0231, 0.1675](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.3.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0334, 0.7964](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.3.self_attn.q_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.k_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.3.self_attn.v_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.3.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.3.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.3.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0462, 0.9634](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.3.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.3.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.3.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0216, 0.6997](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.3.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.3.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.3.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0321, 1.5723](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.4.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0300, 0.3755](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.4.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0236, 0.4360](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.4.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0160, 0.1953](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.4.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0298, 0.6880](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.4.self_attn.q_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.k_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.4.self_attn.v_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.4.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.4.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.4.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0640, 0.9155](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.4.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.4.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.4.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0282, 0.8799](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.4.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.4.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.4.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0142, 1.8203](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.5.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0200, 0.4324](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.5.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0190, 0.4028](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.5.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0218, 0.2744](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.5.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0249, 0.6045](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.5.self_attn.q_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.k_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.5.self_attn.v_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.5.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.5.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.5.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0546, 0.8950](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.5.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.5.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.5.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0208, 0.5200](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.5.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.5.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.5.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0227, 1.8125](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.6.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0120, 0.9849](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.6.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0203, 0.5791](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.6.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0202, 0.2220](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.6.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0306, 0.5811](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.6.self_attn.q_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.k_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.6.self_attn.v_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.6.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.6.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.6.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0698, 1.2334](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.6.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.6.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.6.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0291, 0.5781](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.6.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.6.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.6.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0322, 1.4277](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.7.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0143, 0.6479](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.7.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0274, 0.3438](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.7.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0170, 0.1390](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.7.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0284, 0.9639](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.7.self_attn.q_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.k_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.7.self_attn.v_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.7.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.7.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.7.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0489, 1.2500](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.7.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.7.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.7.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0293, 0.6680](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.7.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.7.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.7.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0255, 1.8252](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.8.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0163, 1.3545](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.8.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0258, 0.6274](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.8.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0233, 0.2322](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.8.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0314, 0.5635](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.8.self_attn.q_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.k_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.8.self_attn.v_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.8.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.8.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.8.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0554, 0.8564](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.8.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.8.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.8.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0241, 0.6299](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.8.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.8.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.8.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0288, 1.7900](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.9.self_attn.q_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.q_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.q_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0175, 0.5122](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.9.self_attn.k_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.k_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.k_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0185, 0.3037](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.9.self_attn.v_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.v_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.v_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0179, 0.2700](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.9.self_attn.o_proj.input_quantizer                                  TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.o_proj.output_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.o_proj.weight_quantizer                                 TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0316, 1.7861](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.9.self_attn.q_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.k_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.9.self_attn.v_bmm_quantizer                                         TensorQuantizer(disabled)\n",
      "model.layers.9.mlp.gate_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.9.mlp.gate_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.9.mlp.gate_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0740, 1.9814](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.9.mlp.up_proj.input_quantizer                                       TensorQuantizer(disabled)\n",
      "model.layers.9.mlp.up_proj.output_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.9.mlp.up_proj.weight_quantizer                                      TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0376, 0.7217](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.9.mlp.down_proj.input_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.9.mlp.down_proj.output_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.9.mlp.down_proj.weight_quantizer                                    TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0252, 1.6357](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.10.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0222, 0.5732](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.10.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0264, 0.4771](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.10.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0228, 0.2177](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.10.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0255, 1.3652](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.10.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.10.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.10.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.10.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.10.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0497, 1.4326](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.10.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.10.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.10.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0346, 0.6719](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.10.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.10.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.10.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0331, 1.4531](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.11.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0121, 0.2583](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.11.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0294, 0.4031](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.11.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0194, 0.2106](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.11.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0252, 1.3311](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.11.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.11.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.11.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.11.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.11.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0571, 1.1816](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.11.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.11.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.11.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0417, 0.6641](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.11.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.11.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.11.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0322, 1.9473](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.12.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0234, 0.8022](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.12.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0220, 0.6641](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.12.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0221, 0.2607](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.12.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0314, 1.1006](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.12.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.12.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.12.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.12.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.12.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0423, 1.3154](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.12.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.12.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.12.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0364, 0.7778](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.12.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.12.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.12.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0277, 1.6240](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.13.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0142, 0.4604](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.13.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0188, 0.3003](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.13.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0193, 0.2693](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.13.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0295, 0.8154](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.13.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.13.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.13.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.13.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.13.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0447, 1.0508](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.13.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.13.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.13.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0296, 0.5586](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.13.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.13.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.13.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0204, 1.5039](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.14.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0127, 0.6655](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.14.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0147, 1.2900](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.14.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0218, 0.2153](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.14.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0299, 1.1553](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.14.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.14.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.14.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.14.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.14.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0468, 1.1494](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.14.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.14.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.14.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0374, 0.8662](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.14.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.14.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.14.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0386, 1.6367](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.15.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0143, 0.8550](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.15.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0308, 0.4172](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.15.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0225, 0.2332](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.15.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0225, 1.3848](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.15.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.15.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.15.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.15.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.15.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0398, 0.7505](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.15.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.15.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.15.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0334, 0.5288](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.15.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.15.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.15.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0285, 2.5605](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.16.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0240, 0.3772](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.16.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0222, 0.6753](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.16.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0217, 0.2472](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.16.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0310, 1.4053](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.16.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.16.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.16.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.16.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.16.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0511, 1.1738](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.16.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.16.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.16.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0441, 0.7974](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.16.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.16.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.16.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0334, 2.1836](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.17.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0164, 1.3369](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.17.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0255, 0.7651](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.17.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0234, 0.2959](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.17.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0293, 1.2920](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.17.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.17.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.17.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.17.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.17.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0407, 1.0635](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.17.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.17.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.17.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0354, 0.8794](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.17.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.17.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.17.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0305, 1.7217](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.18.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0128, 0.5454](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.18.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0202, 0.6909](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.18.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0246, 0.1449](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.18.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0306, 0.7524](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.18.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.18.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.18.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.18.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.18.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0402, 0.7651](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.18.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.18.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.18.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0324, 0.6235](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.18.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.18.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.18.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0272, 2.3730](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.19.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0176, 0.3540](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.19.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0220, 0.9941](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.19.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0247, 0.1565](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.19.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0399, 1.4805](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.19.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.19.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.19.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.19.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.19.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0492, 1.0391](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.19.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.19.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.19.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0373, 0.6685](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.19.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.19.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.19.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0367, 2.0176](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.20.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0209, 0.3350](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.20.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0205, 0.3250](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.20.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0326, 0.1692](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.20.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0332, 1.1689](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.20.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.20.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.20.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.20.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.20.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0681, 1.2842](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.20.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.20.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.20.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0421, 0.7417](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.20.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.20.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.20.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0334, 1.7637](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.21.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0182, 0.3132](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.21.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0199, 0.5049](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.21.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0203, 0.1477](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.21.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0318, 0.7993](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.21.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.21.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.21.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.21.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.21.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0501, 1.1787](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.21.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.21.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.21.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0354, 0.6348](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.21.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.21.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.21.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0285, 1.8584](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.22.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0166, 0.4448](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.22.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0176, 0.3047](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.22.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0299, 0.2454](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.22.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0383, 0.8325](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.22.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.22.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.22.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.22.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.22.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0421, 1.2148](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.22.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.22.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.22.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0245, 0.6211](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.22.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.22.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.22.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0247, 2.2695](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.23.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0114, 0.4675](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.23.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0211, 0.3945](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.23.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0198, 0.1860](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.23.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0285, 0.7075](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.23.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.23.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.23.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.23.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.23.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0593, 1.0098](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.23.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.23.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.23.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0363, 0.6719](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.23.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.23.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.23.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0181, 2.7832](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.24.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0225, 0.3821](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.24.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0199, 0.3916](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.24.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0188, 0.1644](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.24.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0286, 0.9722](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.24.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.24.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.24.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.24.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.24.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0630, 1.0068](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.24.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.24.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.24.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0366, 0.4883](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.24.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.24.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.24.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0181, 1.8379](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.25.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0193, 0.3413](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.25.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0265, 0.3137](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.25.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0279, 0.1536](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.25.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0403, 1.0557](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.25.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.25.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.25.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.25.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.25.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0660, 1.3213](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.25.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.25.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.25.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0365, 0.6846](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.25.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.25.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.25.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0113, 2.7715](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.26.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0190, 0.2808](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.26.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0226, 0.3193](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.26.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0279, 0.1857](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.26.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0316, 1.2148](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.26.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.26.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.26.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.26.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.26.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0389, 0.8384](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.26.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.26.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.26.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0314, 0.7231](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.26.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.26.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.26.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0071, 3.9219](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.27.self_attn.q_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.q_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.q_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0212, 0.2908](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.27.self_attn.k_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.k_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.k_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0179, 0.2710](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.27.self_attn.v_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.v_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.v_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0230, 0.3188](3072) calibrator=MaxCalibrator quant)\n",
      "model.layers.27.self_attn.o_proj.input_quantizer                                 TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.o_proj.output_quantizer                                TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.o_proj.weight_quantizer                                TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0239, 2.5254](18432) calibrator=MaxCalibrator quant)\n",
      "model.layers.27.self_attn.q_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.k_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.27.self_attn.v_bmm_quantizer                                        TensorQuantizer(disabled)\n",
      "model.layers.27.mlp.gate_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.27.mlp.gate_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.27.mlp.gate_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0687, 1.5625](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.27.mlp.up_proj.input_quantizer                                      TensorQuantizer(disabled)\n",
      "model.layers.27.mlp.up_proj.output_quantizer                                     TensorQuantizer(disabled)\n",
      "model.layers.27.mlp.up_proj.weight_quantizer                                     TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0414, 1.1963](107520) calibrator=MaxCalibrator quant)\n",
      "model.layers.27.mlp.down_proj.input_quantizer                                    TensorQuantizer(disabled)\n",
      "model.layers.27.mlp.down_proj.output_quantizer                                   TensorQuantizer(disabled)\n",
      "model.layers.27.mlp.down_proj.weight_quantizer                                   TensorQuantizer(4 bit fake block_sizes={-1: 128, 'type': 'static'}, amax=[0.0249, 6.5391](107520) calibrator=MaxCalibrator quant)\n",
      "lm_head.input_quantizer                                                          TensorQuantizer(disabled)\n",
      "lm_head.output_quantizer                                                         TensorQuantizer(disabled)\n",
      "lm_head.weight_quantizer                                                         TensorQuantizer(disabled)\n",
      "675 TensorQuantizers found in model\n"
     ]
    }
   ],
   "source": [
    "mtq.print_quant_summary(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e5b037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================================================================================================\n",
       "Layer (type:depth-idx)                                  Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
       "====================================================================================================================================================================================\n",
       "Qwen2ForCausalLM                                        [1, 128]                  --                        --                        --                        --\n",
       "├─Qwen2Model: 1-1                                       --                        --                        --                        --                        --\n",
       "│    └─Embedding: 2-1                                   [1, 128]                  [1, 128, 1536]            233,373,696               --                        233,373,696\n",
       "│    └─Qwen2RotaryEmbedding: 2-2                        [1, 128, 1536]            [1, 128, 128]             --                        --                        --\n",
       "│    └─ModuleList: 2-3                                  --                        --                        --                        --                        --\n",
       "│    │    └─Qwen2DecoderLayer: 3-1                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-2                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-3                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-4                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-5                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-6                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-7                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-8                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-9                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-10                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-11                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-12                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-13                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-14                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-15                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-16                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-17                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-18                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-19                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-20                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-21                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-22                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-23                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-24                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-25                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-26                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-27                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-28                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    └─Qwen2RMSNorm: 2-4                                [1, 128, 1536]            [1, 128, 1536]            1,536                     --                        1,536\n",
       "├─QuantLinear: 1-2                                      [1, 128, 1536]            [1, 128, 151936]          233,373,696               --                        --\n",
       "│    └─TensorQuantizer: 2-5                             [1, 128, 1536]            [1, 128, 1536]            --                        --                        --\n",
       "│    └─TensorQuantizer: 2-6                             [151936, 1536]            [151936, 1536]            --                        --                        --\n",
       "│    └─TensorQuantizer: 2-7                             [1, 128, 151936]          [1, 128, 151936]          --                        --                        --\n",
       "====================================================================================================================================================================================\n",
       "Total params: 1,777,088,000\n",
       "Trainable params: 1,777,088,000\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 233.46\n",
       "====================================================================================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 45.61\n",
       "Params size (MB): 466.92\n",
       "Estimated Total Size (MB): 512.54\n",
       "===================================================================================================================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len  = 128\n",
    "batch    = torch.randint(\n",
    "              0, qmodel.config.vocab_size,        # token IDs\n",
    "              (1, seq_len),                       # (B, L)\n",
    "              dtype=torch.long,\n",
    "              device=device)\n",
    "summary(qmodel, input_data=batch, col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17efe6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================================================================================================\n",
       "Layer (type:depth-idx)                                  Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
       "====================================================================================================================================================================================\n",
       "Qwen2ForCausalLM                                        [1, 128]                  --                        --                        --                        --\n",
       "├─Qwen2Model: 1-1                                       --                        --                        --                        --                        --\n",
       "│    └─Embedding: 2-1                                   [1, 128]                  [1, 128, 1536]            233,373,696               --                        233,373,696\n",
       "│    └─Qwen2RotaryEmbedding: 2-2                        [1, 128, 1536]            [1, 128, 128]             --                        --                        --\n",
       "│    └─ModuleList: 2-3                                  --                        --                        --                        --                        --\n",
       "│    │    └─Qwen2DecoderLayer: 3-1                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-2                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-3                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-4                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-5                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-6                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-7                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-8                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-9                      [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-10                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-11                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-12                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-13                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-14                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-15                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-16                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-17                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-18                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-19                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-20                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-21                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-22                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-23                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-24                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-25                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-26                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-27                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    │    └─Qwen2DecoderLayer: 3-28                     [1, 128, 1536]            [1, 128, 1536]            46,797,824                --                        3,072\n",
       "│    └─Qwen2RMSNorm: 2-4                                [1, 128, 1536]            [1, 128, 1536]            1,536                     --                        1,536\n",
       "├─QuantLinear: 1-2                                      [1, 128, 1536]            [1, 128, 151936]          233,373,696               --                        --\n",
       "│    └─TensorQuantizer: 2-5                             [1, 128, 1536]            [1, 128, 1536]            --                        --                        --\n",
       "│    └─TensorQuantizer: 2-6                             [151936, 1536]            [151936, 1536]            --                        --                        --\n",
       "│    └─TensorQuantizer: 2-7                             [1, 128, 151936]          [1, 128, 151936]          --                        --                        --\n",
       "====================================================================================================================================================================================\n",
       "Total params: 1,777,088,000\n",
       "Trainable params: 1,777,088,000\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 233.46\n",
       "====================================================================================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 45.61\n",
       "Params size (MB): 466.92\n",
       "Estimated Total Size (MB): 512.54\n",
       "===================================================================================================================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len  = 128\n",
    "batch    = torch.randint(\n",
    "              0, model_pre_q.config.vocab_size,        # token IDs\n",
    "              (1, seq_len),                       # (B, L)\n",
    "              dtype=torch.long,\n",
    "              device=device)\n",
    "summary(model_pre_q, input_data=batch, col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2b0b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtq.compress(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21696d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
